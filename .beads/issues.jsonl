{"id":"RAG-6gf","title":"PDF-zu-MD Pipeline mit MinerU integrieren","description":"MinerU (${MINERU_PATH}) für PDF→MD Konvertierung. Klären: Wie Dateinamen/Quellzuordnung in Vector DB tracken?","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T17:45:25.939956+01:00","updated_at":"2025-12-29T00:20:53.323632+01:00","closed_at":"2025-12-29T00:20:53.323632+01:00","close_reason":"Pipeline-Refactoring abgeschlossen. PDF -\u003e MD -\u003e chunks.json -\u003e pgvector Flow implementiert.","comments":[{"id":1,"issue_id":"RAG-6gf","author":"brunowinter2000","text":"Slash Command pdf-to-rag.md + md-cleanup agent erstellt. Test mit echter PDF offen.","created_at":"2025-12-28T17:33:00Z"},{"id":2,"issue_id":"RAG-6gf","author":"brunowinter2000","text":"Pre-Cleanup (postprocess.py) + Chunk-basiertes LLM-Cleanup implementiert. Mineru workflow.py erstellt. Test mit echter PDF noch offen.","created_at":"2025-12-28T18:38:10Z"}]}
{"id":"RAG-koy","title":"README Tech Stack dokumentieren","description":"Docker, PostgreSQL+pgvector, Qwen3-Embedding-8B, llama.cpp Server, FastMCP - Stack in README beschreiben.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T17:45:31.475547+01:00","updated_at":"2025-12-28T23:21:21.723923+01:00","closed_at":"2025-12-28T23:21:21.723923+01:00","close_reason":"README.md mit vollständigem Tech Stack aktualisiert"}
{"id":"RAG-sgg","title":"RAG Indexing: llama.cpp Stability","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-29T15:59:45.17265+01:00","updated_at":"2025-12-30T00:03:15.336751+01:00","closed_at":"2025-12-30T00:03:15.336751+01:00","close_reason":"Fixed llama.cpp crash: -ub 4096, token-based truncation, delete_source() for duplicates, removed legacy code","comments":[{"id":3,"issue_id":"RAG-sgg","author":"brunowinter2000","text":"llama.cpp embedding server crashes after ~220 requests. Issues:\n1. Server crashes mid-indexing (222/413 chunks)\n2. Abort trap 6 on large batches\n3. Need retry logic or chunked restart\n\nWorkarounds tried:\n- BATCH_SIZE=1\n- MAX_CHUNK_CHARS=2000 truncation\n- Server restart between runs\n\nNext steps:\n- Add retry with server restart\n- Or use different embedding backend (Ollama?)\n- Or batch with sleep between requests","created_at":"2025-12-29T14:59:56Z"},{"id":4,"issue_id":"RAG-sgg","author":"brunowinter2000","text":"## Investigation Notes (2025-12-29)\n\n### Symptom\n- llama-server crashes after ~200-220 embedding requests\n- Error: httpx.RemoteProtocolError: Server disconnected without sending a response\n- Happens consistently during batch indexing\n\n### Server Config\n- Model: Qwen3-Embedding-8B-Q5_K_M.gguf (5GB)\n- Command: llama-server -m model.gguf --embedding --host 0.0.0.0 --port 8081\n- Auto-config: n_ctx=40960, n_slots=4, n_batch=512\n\n### Memory Usage (from startup logs)\n- Model buffer: 5165 MiB (Metal) + 486 MiB (CPU)\n- KV Cache: 5760 MiB (seems excessive for embedding-only mode)\n- Total: ~11GB just for model + KV cache\n\n### Hypothesis\n1. KV cache fills up despite embedding mode (shouldn't need KV cache)\n2. Memory leak in llama.cpp embedding endpoint\n3. n_ctx=40960 is overkill, causing memory pressure\n\n### Potential Fixes to Test\n1. Set smaller n_ctx (e.g. --ctx-size 2048)\n2. Disable KV cache if possible for embeddings\n3. Reduce n_slots (--parallel 1)\n4. Check llama.cpp issues for embedding server memory leaks\n\n### Workaround Implemented\n- embedder.py: Auto-restart on crash (MAX_RESTART_ATTEMPTS=10)\n- Added RemoteProtocolError to exception handling\n- Works but ugly - need root cause fix","created_at":"2025-12-29T17:43:42Z"},{"id":5,"issue_id":"RAG-sgg","author":"brunowinter2000","text":"## Update (2025-12-29)\n\nWorkaround (auto-restart) REVERTED - nicht der richtige Ansatz.\n\nNächster Schritt: Root Cause Investigation\n- Warum crashed llama-server nach ~200 Requests?\n- KV Cache 5.7GB trotz embedding-only mode - verdächtig\n- Test mit --ctx-size 2048 --parallel 1\n- llama.cpp GitHub Issues checken","created_at":"2025-12-29T20:01:48Z"}]}
{"id":"RAG-vua","title":"Native arm64 llama.cpp Image","description":"amd64 Image läuft unter Rosetta-Emulation, sehr langsam. Natives arm64 Image bauen oder llama.cpp lokal kompilieren für Performance.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T17:45:18.239819+01:00","updated_at":"2025-12-28T23:21:11.70948+01:00","closed_at":"2025-12-28T23:21:11.70948+01:00","close_reason":"Native Metal GPU build funktioniert, 300x Performance-Gewinn (0.18s vs 60s+)"}
