{"id":"RAG-6gf","title":"PDF-zu-MD Pipeline mit MinerU integrieren","description":"MinerU (${MINERU_PATH}) für PDF→MD Konvertierung. Klären: Wie Dateinamen/Quellzuordnung in Vector DB tracken?","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T17:45:25.939956+01:00","updated_at":"2025-12-29T00:20:53.323632+01:00","closed_at":"2025-12-29T00:20:53.323632+01:00","close_reason":"Pipeline-Refactoring abgeschlossen. PDF -\u003e MD -\u003e chunks.json -\u003e pgvector Flow implementiert.","comments":[{"id":1,"issue_id":"RAG-6gf","author":"brunowinter2000","text":"Slash Command pdf-to-rag.md + md-cleanup agent erstellt. Test mit echter PDF offen.","created_at":"2025-12-28T17:33:00Z"},{"id":2,"issue_id":"RAG-6gf","author":"brunowinter2000","text":"Pre-Cleanup (postprocess.py) + Chunk-basiertes LLM-Cleanup implementiert. Mineru workflow.py erstellt. Test mit echter PDF noch offen.","created_at":"2025-12-28T18:38:10Z"}]}
{"id":"RAG-koy","title":"README Tech Stack dokumentieren","description":"Docker, PostgreSQL+pgvector, Qwen3-Embedding-8B, llama.cpp Server, FastMCP - Stack in README beschreiben.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T17:45:31.475547+01:00","updated_at":"2025-12-28T23:21:21.723923+01:00","closed_at":"2025-12-28T23:21:21.723923+01:00","close_reason":"README.md mit vollständigem Tech Stack aktualisiert"}
{"id":"RAG-sgg","title":"RAG Indexing: llama.cpp Stability","status":"open","priority":2,"issue_type":"bug","created_at":"2025-12-29T15:59:45.17265+01:00","updated_at":"2025-12-29T15:59:45.17265+01:00","comments":[{"id":3,"issue_id":"RAG-sgg","author":"brunowinter2000","text":"llama.cpp embedding server crashes after ~220 requests. Issues:\n1. Server crashes mid-indexing (222/413 chunks)\n2. Abort trap 6 on large batches\n3. Need retry logic or chunked restart\n\nWorkarounds tried:\n- BATCH_SIZE=1\n- MAX_CHUNK_CHARS=2000 truncation\n- Server restart between runs\n\nNext steps:\n- Add retry with server restart\n- Or use different embedding backend (Ollama?)\n- Or batch with sleep between requests","created_at":"2025-12-29T14:59:56Z"}]}
{"id":"RAG-vua","title":"Native arm64 llama.cpp Image","description":"amd64 Image läuft unter Rosetta-Emulation, sehr langsam. Natives arm64 Image bauen oder llama.cpp lokal kompilieren für Performance.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T17:45:18.239819+01:00","updated_at":"2025-12-28T23:21:11.70948+01:00","closed_at":"2025-12-28T23:21:11.70948+01:00","close_reason":"Native Metal GPU build funktioniert, 300x Performance-Gewinn (0.18s vs 60s+)"}
