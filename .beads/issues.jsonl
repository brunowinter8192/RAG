{"id":"RAG-16r","title":"PDF Pipeline: In-place cleanup statt raw+cleaned","description":"Wenn md-cleanup-master Agent stabil: raw.md direkt cleanen statt cleaned.md erstellen. Spart Speicher, weniger Duplikation.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-30T18:54:51.738432+01:00","updated_at":"2025-12-30T18:54:51.738432+01:00"}
{"id":"RAG-5cg","title":"Indexer: BATCH_SIZE erhöhen","description":"BATCH_SIZE in indexer.py von 1 auf höheren Wert erhöhen. Aktuell: jeder Chunk einzeln embedded = langsam.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T18:54:51.263955+01:00","updated_at":"2025-12-31T02:23:14.032242+01:00","closed_at":"2025-12-31T02:23:14.032242+01:00","close_reason":"BATCH_SIZE auf 32 erhöht, Hardware-Info in README"}
{"id":"RAG-67k","title":"Llama Embedding Server Check","description":"Check llama.cpp embedding server performance and configuration. Current: BATCH_SIZE=1, 413 chunks took significant time.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T18:29:28.405237+01:00","updated_at":"2025-12-31T02:08:48.468875+01:00","closed_at":"2025-12-31T02:08:48.468875+01:00","close_reason":"Upgraded to Q8_0 model + added -ngl 99 GPU offload. SOTA config documented in README."}
{"id":"RAG-6gf","title":"PDF-zu-MD Pipeline mit MinerU integrieren","description":"MinerU (${MINERU_PATH}) für PDF→MD Konvertierung. Klären: Wie Dateinamen/Quellzuordnung in Vector DB tracken?","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T17:45:25.939956+01:00","updated_at":"2025-12-29T00:20:53.323632+01:00","closed_at":"2025-12-29T00:20:53.323632+01:00","close_reason":"Pipeline-Refactoring abgeschlossen. PDF -\u003e MD -\u003e chunks.json -\u003e pgvector Flow implementiert.","comments":[{"id":1,"issue_id":"RAG-6gf","author":"brunowinter2000","text":"Slash Command pdf-to-rag.md + md-cleanup agent erstellt. Test mit echter PDF offen.","created_at":"2025-12-28T17:33:00Z"},{"id":2,"issue_id":"RAG-6gf","author":"brunowinter2000","text":"Pre-Cleanup (postprocess.py) + Chunk-basiertes LLM-Cleanup implementiert. Mineru workflow.py erstellt. Test mit echter PDF noch offen.","created_at":"2025-12-28T18:38:10Z"}]}
{"id":"RAG-6u7","title":"workflow.py CLI: --collection, --document args","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-01T03:41:05.595577+01:00","updated_at":"2026-01-02T00:33:54.421803+01:00","closed_at":"2026-01-02T00:33:54.421803+01:00","close_reason":"Implemented --collection and --document filters for search command","comments":[{"id":8,"issue_id":"RAG-6u7","author":"brunowinter2000","text":"MCP Tool hat --collection und --document Filter. workflow.py CLI sollte diese auch haben für Konsistenz.","created_at":"2026-01-01T02:41:10Z"}]}
{"id":"RAG-8if","title":"docker-compose: pgvector Extension auto-enable","description":"Init-Script in docker-compose.yml hinzufügen das CREATE EXTENSION vector automatisch ausführt bei DB-Start.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T18:54:51.500038+01:00","updated_at":"2025-12-31T02:17:42.395055+01:00","closed_at":"2025-12-31T02:17:42.395055+01:00","close_reason":"Won't fix - Volume existiert bereits, Extension manuell aktiviert. Kein akuter Bedarf."}
{"id":"RAG-jsv","title":"Mineru MLX Backend + workflow.py CLI-Flags","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-02T19:02:52.380573+01:00","updated_at":"2026-01-02T19:02:52.380573+01:00","comments":[{"id":11,"issue_id":"RAG-jsv","author":"brunowinter2000","text":"Test vlm-mlx-engine backend für M4 Pro. Dann workflow.py erweitern: --backend, --method, --lang Flags exposen. Expected: 2-5x speedup.","created_at":"2026-01-02T18:03:03Z"}]}
{"id":"RAG-koy","title":"README Tech Stack dokumentieren","description":"Docker, PostgreSQL+pgvector, Qwen3-Embedding-8B, llama.cpp Server, FastMCP - Stack in README beschreiben.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T17:45:31.475547+01:00","updated_at":"2025-12-28T23:21:21.723923+01:00","closed_at":"2025-12-28T23:21:21.723923+01:00","close_reason":"README.md mit vollständigem Tech Stack aktualisiert"}
{"id":"RAG-peu","title":"RAG Indexer Speedup Validation (Benchmark)","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-02T19:02:52.603528+01:00","updated_at":"2026-01-02T19:02:52.603528+01:00","comments":[{"id":12,"issue_id":"RAG-peu","author":"brunowinter2000","text":"embedder.py fix: tokenize API calls entfernt, char-basierte Schätzung statt dessen. Benchmark: Re-run index-json mit 9452 chunks, compare old (~45min) vs new (expected 5-10min).","created_at":"2026-01-02T18:03:03Z"}]}
{"id":"RAG-qj3","title":"md-cleanup edge cases","description":"Remaining issues in cleaned.md after md-cleanup-master run:\n\nOCR Issues (5):\n- Line 982: 1_suppkey → l_suppkey\n- Line 1130: 1_orderkey, 1_receiptdate\n- Line 1467: 11.1_orderkey alias corruption  \n- Line 1898: 0_KEY → O_KEY\n- Line 3600: 1 commitdate → l_commitdate\n\nLaTeX (2):\n- Line 2290: mathbf xi in SQL\n- Line 2722: Throughput formula\n\nFix options: iterate md-cleanup, extend clean_pdf.py, or defer to per-chunk LLM","status":"open","priority":2,"issue_type":"bug","created_at":"2025-12-30T01:23:45.734315+01:00","updated_at":"2025-12-31T03:06:49.539722+01:00","comments":[{"id":6,"issue_id":"RAG-qj3","author":"brunowinter2000","text":"## Reproducible Test Procedure\n\n### Setup\n```bash\n# Ensure clean state\nrm -f data/documents/specification/raw_cleaned.md\nrm -f data/documents/specification/cleaned.md\nls data/documents/specification/  # Should only show: chunks.json, chunks_raw.json, raw.md\n```\n\n### Test Run\n```bash\n# Launch md-cleanup-master with new constraints\n# (Prompt template from agent-dispatch SKILL.md)\n```\n\nPrompt:\n```\nClean the PDF-converted markdown at ./data/documents/specification/raw.md\n\nSAFETY CONSTRAINTS:\n1. Before ANY changes: verify line count (wc -l). After changes: check again.\n   If line count drops \u003e1%, REVERT immediately (git checkout).\n2. Do NOT read full file (\u003e250KB). Use grep -nC 3 to inspect patterns locally.\n3. Regex MUST be surgical: use word boundaries (\\b1_orderkey\\b → l_orderkey).\n   NEVER use greedy wildcards (.*) that span lines.\n4. After cleanup: verify SQL syntax intact (WHERE x = y operators preserved).\n\nKnown issues:\n- OCR: 1/l confusion (1_suppkey → l_suppkey, 1_orderkey → l_orderkey)\n- OCR: 0/O confusion (0_KEY → O_KEY)\n- LaTeX: mathbf, prime remnants in SQL\n```\n\n### Validation\n```bash\n# Check remaining issues\ngrep -c \"1_suppkey\\|1_orderkey\\|0_KEY\\|mathbf\" data/documents/specification/raw_cleaned.md\n\n# Line count integrity\nwc -l data/documents/specification/raw.md\nwc -l data/documents/specification/raw_cleaned.md\n```\n\n### Success Criteria\n- Line count: ±1% of original (3842 lines)\n- OCR issues: 0 (was 5)\n- LaTeX issues: ≤2 (was 7)\n- No trial-and-error loops (agent should succeed on first attempt)","created_at":"2025-12-31T01:44:57Z"},{"id":7,"issue_id":"RAG-qj3","author":"brunowinter2000","text":"Ongoing: Dokumentiert md-cleanup Versuchsaufbau. TPC-H spec cleaned, weitere MDs ausstehend.","created_at":"2025-12-31T02:06:56Z"},{"id":10,"issue_id":"RAG-qj3","author":"brunowinter2000","text":"Fixed in SKILL.md + md-cleanup.md: (1) LaTeX now UNWRAPs arguments instead of deleting them, (2) Dictionary now uses /usr/share/dict/words + document vocab instead of hardcoded 50-word list","created_at":"2026-01-02T18:01:00Z"}]}
{"id":"RAG-sgg","title":"RAG Indexing: llama.cpp Stability","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-29T15:59:45.17265+01:00","updated_at":"2025-12-30T00:03:15.336751+01:00","closed_at":"2025-12-30T00:03:15.336751+01:00","close_reason":"Fixed llama.cpp crash: -ub 4096, token-based truncation, delete_source() for duplicates, removed legacy code","comments":[{"id":3,"issue_id":"RAG-sgg","author":"brunowinter2000","text":"llama.cpp embedding server crashes after ~220 requests. Issues:\n1. Server crashes mid-indexing (222/413 chunks)\n2. Abort trap 6 on large batches\n3. Need retry logic or chunked restart\n\nWorkarounds tried:\n- BATCH_SIZE=1\n- MAX_CHUNK_CHARS=2000 truncation\n- Server restart between runs\n\nNext steps:\n- Add retry with server restart\n- Or use different embedding backend (Ollama?)\n- Or batch with sleep between requests","created_at":"2025-12-29T14:59:56Z"},{"id":4,"issue_id":"RAG-sgg","author":"brunowinter2000","text":"## Investigation Notes (2025-12-29)\n\n### Symptom\n- llama-server crashes after ~200-220 embedding requests\n- Error: httpx.RemoteProtocolError: Server disconnected without sending a response\n- Happens consistently during batch indexing\n\n### Server Config\n- Model: Qwen3-Embedding-8B-Q5_K_M.gguf (5GB)\n- Command: llama-server -m model.gguf --embedding --host 0.0.0.0 --port 8081\n- Auto-config: n_ctx=40960, n_slots=4, n_batch=512\n\n### Memory Usage (from startup logs)\n- Model buffer: 5165 MiB (Metal) + 486 MiB (CPU)\n- KV Cache: 5760 MiB (seems excessive for embedding-only mode)\n- Total: ~11GB just for model + KV cache\n\n### Hypothesis\n1. KV cache fills up despite embedding mode (shouldn't need KV cache)\n2. Memory leak in llama.cpp embedding endpoint\n3. n_ctx=40960 is overkill, causing memory pressure\n\n### Potential Fixes to Test\n1. Set smaller n_ctx (e.g. --ctx-size 2048)\n2. Disable KV cache if possible for embeddings\n3. Reduce n_slots (--parallel 1)\n4. Check llama.cpp issues for embedding server memory leaks\n\n### Workaround Implemented\n- embedder.py: Auto-restart on crash (MAX_RESTART_ATTEMPTS=10)\n- Added RemoteProtocolError to exception handling\n- Works but ugly - need root cause fix","created_at":"2025-12-29T17:43:42Z"},{"id":5,"issue_id":"RAG-sgg","author":"brunowinter2000","text":"## Update (2025-12-29)\n\nWorkaround (auto-restart) REVERTED - nicht der richtige Ansatz.\n\nNächster Schritt: Root Cause Investigation\n- Warum crashed llama-server nach ~200 Requests?\n- KV Cache 5.7GB trotz embedding-only mode - verdächtig\n- Test mit --ctx-size 2048 --parallel 1\n- llama.cpp GitHub Issues checken","created_at":"2025-12-29T20:01:48Z"}]}
{"id":"RAG-vua","title":"Native arm64 llama.cpp Image","description":"amd64 Image läuft unter Rosetta-Emulation, sehr langsam. Natives arm64 Image bauen oder llama.cpp lokal kompilieren für Performance.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T17:45:18.239819+01:00","updated_at":"2025-12-28T23:21:11.70948+01:00","closed_at":"2025-12-28T23:21:11.70948+01:00","close_reason":"Native Metal GPU build funktioniert, 300x Performance-Gewinn (0.18s vs 60s+)"}
{"id":"RAG-xxg","title":"md-cleanup Polish: Attitude section + Python template","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-02T02:40:08.820074+01:00","updated_at":"2026-01-02T02:40:58.985692+01:00","closed_at":"2026-01-02T02:40:58.985692+01:00","close_reason":"Attitude section added to md-cleanup.md"}
{"id":"RAG-zbl","title":"Mineru output path cleanup","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-02T01:41:51.390746+01:00","updated_at":"2026-01-02T01:41:51.390746+01:00","comments":[{"id":9,"issue_id":"RAG-zbl","author":"brunowinter2000","text":"Mineru creates output in ${MINERU_PATH}/output/ but should ONLY output to RAG/data/documents/{stem}/raw/. Either delete the Mineru output folder or ensure the workflow clears it after each run.","created_at":"2026-01-02T00:41:58Z"}]}
